{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Setting up the environment"
   ],
   "metadata": {
    "id": "8GVq5fs3Iare"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## clone the project"
   ],
   "metadata": {
    "id": "eMiZg-dHOjTt"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFtmSKcVIAj9",
    "outputId": "b282b74f-3f2b-4fbe-c2a9-bf2e4d99af6e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "/content/drive/MyDrive\n",
      "/content/drive/MyDrive/codes\n",
      "/content/drive/MyDrive/codes/logic_based_qa\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/\n",
    "!mkdir -p codes\n",
    "%cd /content/drive/MyDrive/codes\n",
    "# !git clone https://github.com/navidmdn/logic_based_qa.git\n",
    "%cd logic_based_qa"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## install requirements and SWI prolog"
   ],
   "metadata": {
    "id": "Aopmu8EyOyjr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eQt_5H0VIZpP",
    "outputId": "933d300e-f62f-447a-de30-c8a3c8f78406"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting pyswip\n",
      "  Cloning https://github.com/yuce/pyswip (to revision master) to /tmp/pip-install-j8wiud06/pyswip_ab6c177004f04ed6bcd49c9c028e07ae\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/yuce/pyswip /tmp/pip-install-j8wiud06/pyswip_ab6c177004f04ed6bcd49c9c028e07ae\n",
      "  Resolved https://github.com/yuce/pyswip to commit 59016e0841f56177d1b18ec08fd9b67792bd0a97\n",
      "  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 1)) (1.10.0)\n",
      "Requirement already satisfied: tqdm==4.62.3 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 2)) (4.62.3)\n",
      "Requirement already satisfied: tensorboard==2.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 4)) (2.7.0)\n",
      "Requirement already satisfied: transformers==4.23.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 5)) (4.23.1)\n",
      "Requirement already satisfied: datasets==2.4.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (2.4.0)\n",
      "Requirement already satisfied: evaluate==0.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (0.2.2)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.10.0->-r requirements.txt (line 1)) (4.4.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (0.38.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (1.51.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (3.19.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (2.16.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (1.21.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (57.4.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (1.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard==2.7.0->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (2022.6.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (0.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (0.13.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers==4.23.1->-r requirements.txt (line 5)) (21.3)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (1.3.5)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (3.2.0)\n",
      "Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (0.3.5.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (0.70.13)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (3.8.3)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (9.0.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets==2.4.0->-r requirements.txt (line 6)) (2022.11.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (1.8.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (22.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets==2.4.0->-r requirements.txt (line 6)) (2.1.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 4)) (1.15.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 4)) (5.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 4)) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 4)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 4)) (6.0.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers==4.23.1->-r requirements.txt (line 5)) (3.0.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 4)) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 4)) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 4)) (1.26.14)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard==2.7.0->-r requirements.txt (line 4)) (2.10)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.4.0->-r requirements.txt (line 6)) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==2.4.0->-r requirements.txt (line 6)) (2.8.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard==2.7.0->-r requirements.txt (line 4)) (3.11.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.7.0->-r requirements.txt (line 4)) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.7.0->-r requirements.txt (line 4)) (3.2.2)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!sudo apt-add-repository ppa:swi-prolog/stable\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install swi-prolog"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HVNfWieDN9PY",
    "outputId": "7fb444b5-2952-4224-d1b7-e600f325ce17"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " Comprehensive Prolog implementation with extensive libraries and development tools.   Primarily targetted at teaching, RDF processing and web-related tasks, such as creating web services or analysing web content.\n",
      "\n",
      "Official PPAs for SWI-Prolog. See https://www.swi-prolog.org for further information.\n",
      " More info: https://launchpad.net/~swi-prolog/+archive/ubuntu/stable\n",
      "Press [ENTER] to continue or Ctrl-c to cancel adding it.\n",
      "\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
      "Hit:8 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Get:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n",
      "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Get:12 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2,439 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
      "Hit:15 http://ppa.launchpad.net/swi-prolog/stable/ubuntu focal InRelease\n",
      "Get:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [986 kB]\n",
      "Fetched 3,761 kB in 2s (1,912 kB/s)\n",
      "Reading package lists... Done\n",
      "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
      "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
      "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
      "Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n",
      "Hit:7 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
      "Hit:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
      "Hit:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n",
      "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
      "Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
      "Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
      "Hit:14 http://ppa.launchpad.net/swi-prolog/stable/ubuntu focal InRelease\n",
      "Fetched 108 kB in 2s (71.4 kB/s)\n",
      "Reading package lists... Done\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "swi-prolog is already the newest version (9.0.3-1-gac10f6bcb-focalppa2).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 33 not upgraded.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training language to predicate translator"
   ],
   "metadata": {
    "id": "jBsq6ikFO98h"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## build and save translation dataset"
   ],
   "metadata": {
    "id": "tYigwCw3QJOV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!PYTHONPATH=. python nl2log/data_loader.py --data_path=./data --dataset=metaqa --sample_size 1000"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o8LsCuL4PNtv",
    "outputId": "b96c5b37-549a-4d2b-da05-12ec4d2c415a"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "100% 134741/134741 [00:00<00:00, 253711.42it/s]\n",
      "loaded 269482 triplets with 43234 entities and 18 relations\n",
      "100% 269482/269482 [00:00<00:00, 1141107.13it/s]\n",
      "train set size:329282, test set size: 39093, dev set size: 39138\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## training the translator"
   ],
   "metadata": {
    "id": "pQF_LVbAQVQ2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!bash translation_trainer.sh 1000"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9j9r-Nj0Peak",
    "outputId": "c869bd00-9e1e-48cf-b8bc-ab729138c525"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "Moving 0 files to the new cache system\n",
      "\r0it [00:00, ?it/s]\r0it [00:00, ?it/s]\n",
      "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=True,\n",
      "do_train=True,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=2000,\n",
      "evaluation_strategy=steps,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "generation_max_length=200,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=passive,\n",
      "log_level_replica=passive,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./models/t5-small/runs/Jan25_18-56-20_b10f6aec3a64,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=500,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_hf,\n",
      "output_dir=./models/t5-small,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=64,\n",
      "per_device_train_batch_size=32,\n",
      "predict_with_generate=True,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./models/t5-small,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "tf32=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "WARNING:datasets.builder:Using custom data configuration default-cd6ff4b266072305\n",
      "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
      "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253\n",
      "WARNING:datasets.builder:Reusing dataset json (/root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n",
      "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253\n",
      "100% 3/3 [00:00<00:00, 650.62it/s]\n",
      "[INFO|configuration_utils.py:653] 2023-01-25 18:56:21,436 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/config.json\n",
      "[INFO|configuration_utils.py:705] 2023-01-25 18:56:21,440 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_auto.py:418] 2023-01-25 18:56:21,705 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "[INFO|configuration_utils.py:653] 2023-01-25 18:56:21,971 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/config.json\n",
      "[INFO|configuration_utils.py:705] 2023-01-25 18:56:21,971 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1773] 2023-01-25 18:56:22,504 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1773] 2023-01-25 18:56:22,504 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1773] 2023-01-25 18:56:22,504 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2023-01-25 18:56:22,504 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1773] 2023-01-25 18:56:22,504 >> loading file tokenizer_config.json from cache at None\n",
      "[INFO|configuration_utils.py:653] 2023-01-25 18:56:22,504 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/config.json\n",
      "[INFO|configuration_utils.py:705] 2023-01-25 18:56:22,505 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.23.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/models/t5/tokenization_t5_fast.py:156: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-small automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "[INFO|modeling_utils.py:2156] 2023-01-25 18:56:22,575 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--t5-small/snapshots/3479082dc36f8a4730936ef1c9b88cd8b0835c53/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:2606] 2023-01-25 18:56:23,591 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:2614] 2023-01-25 18:56:23,592 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-b512297dd3938cc6.arrow\n",
      "Running tokenizer on validation dataset:   0% 0/40 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-2fd334b7e6d5f3e3.arrow\n",
      "Running tokenizer on validation dataset: 100% 40/40 [00:04<00:00,  9.69ba/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/json/default-cd6ff4b266072305/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253/cache-6d05c87b1faa3999.arrow\n",
      "Downloading builder script: 100% 5.67k/5.67k [00:00<00:00, 5.48MB/s]\n",
      "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[INFO|trainer.py:1607] 2023-01-25 18:56:33,004 >> ***** Running training *****\n",
      "[INFO|trainer.py:1608] 2023-01-25 18:56:33,004 >>   Num examples = 329282\n",
      "[INFO|trainer.py:1609] 2023-01-25 18:56:33,004 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:1610] 2023-01-25 18:56:33,004 >>   Instantaneous batch size per device = 32\n",
      "[INFO|trainer.py:1611] 2023-01-25 18:56:33,004 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "[INFO|trainer.py:1612] 2023-01-25 18:56:33,004 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1613] 2023-01-25 18:56:33,004 >>   Total optimization steps = 10291\n",
      "  0% 0/10291 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-01-25 18:56:33,053 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.4206, 'learning_rate': 4.7570692838402495e-05, 'epoch': 0.05}\n",
      "  5% 500/10291 [01:39<32:27,  5.03it/s][INFO|trainer.py:2656] 2023-01-25 18:58:12,558 >> Saving model checkpoint to ./models/t5-small/checkpoint-500\n",
      "[INFO|configuration_utils.py:447] 2023-01-25 18:58:12,563 >> Configuration saved in ./models/t5-small/checkpoint-500/config.json\n",
      "[INFO|modeling_utils.py:1624] 2023-01-25 18:58:13,406 >> Model weights saved in ./models/t5-small/checkpoint-500/pytorch_model.bin\n",
      "[INFO|tokenization_utils_base.py:2123] 2023-01-25 18:58:13,447 >> tokenizer config file saved in ./models/t5-small/checkpoint-500/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2130] 2023-01-25 18:58:13,452 >> Special tokens file saved in ./models/t5-small/checkpoint-500/special_tokens_map.json\n",
      "  8% 800/10291 [02:43<32:02,  4.94it/s]Traceback (most recent call last):\n",
      "  File \"nl2log/trainer.py\", line 717, in <module>\n",
      "    main()\n",
      "  File \"nl2log/trainer.py\", line 655, in main\n",
      "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1500, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/trainer.py\", line 1809, in _inner_training_loop\n",
      "    self.optimizer.step()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py\", line 65, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py\", line 88, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/transformers/optimization.py\", line 362, in step\n",
      "    denom = exp_avg_sq.sqrt().add_(group[\"eps\"])\n",
      "KeyboardInterrupt\n",
      "  8% 800/10291 [02:44<32:29,  4.87it/s]\n",
      "^C\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluate the translator model "
   ],
   "metadata": {
    "id": "_c51EkNlSuCO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!PYTHONPATH=. python nl2log/evaluation.py --model_cp=models/t5-small-1000"
   ],
   "metadata": {
    "id": "MlZ6EuI5R9ME"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Question Answering Module"
   ],
   "metadata": {
    "id": "tMo_6xasTUw2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generate predicates from test questions"
   ],
   "metadata": {
    "id": "NP72j2ZjTb05"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!PYTHONPATH=. python qa/evaluation.py --model_path=\"./models/t5-small/checkpoint-500\" --generate_predicates"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E600kF-eTamN",
    "outputId": "cce61c67-427e-433b-e5f9-6add1326616e"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Generating predicates\n",
      "100% 134741/134741 [00:00<00:00, 247241.43it/s]\n",
      "loaded 269482 triplets with 43234 entities and 18 relations\n",
      "100% 269482/269482 [00:00<00:00, 1267720.47it/s]\n",
      "100% 269482/269482 [00:05<00:00, 46406.15it/s]\n",
      "100% 39/39 [00:23<00:00,  1.63it/s]\n",
      "100% 59/59 [00:48<00:00,  1.20it/s]\n",
      "100% 56/56 [01:03<00:00,  1.13s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## evaluating final model"
   ],
   "metadata": {
    "id": "VhnO72MwT2Fh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!PYTHONPATH=. python qa/evaluation.py --model_path=\"./models/t5-small/checkpoint-500\""
   ],
   "metadata": {
    "id": "UgS1med7T1HZ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Manually query model"
   ],
   "metadata": {
    "id": "NT274IxiUg4J"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mHOM0NKIYQZP",
    "outputId": "62f2efee-d113-4d69-bd58-dd48b1ad6968"
   },
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from qa.question_answering import QuestionAnswering\n",
    "from qa.data_loader import MetaQADataLoader\n",
    "\n",
    "data_loader = MetaQADataLoader('./data')\n",
    "qa = QuestionAnswering('./models/t5-small/checkpoint-5000', data_loader)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bg2MPJm5UgPb",
    "outputId": "dc5af8ed-5270-44a7-e843-400c4e7e7f84"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 134741/134741 [00:00<00:00, 174335.64it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "loaded 269482 triplets with 43234 entities and 18 relations\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 269482/269482 [00:00<00:00, 1029146.93it/s]\n",
      "100%|██████████| 269482/269482 [00:08<00:00, 32746.59it/s]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!head -n 5 data/3hop/qa_test.txt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "di83W0V3VnsX",
    "outputId": "d58a9915-1564-4fd6-e6f5-80da5c31dc88"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the films that share directors with the film [Catch Me If You Can] were in which languages\tGerman|Polish|Mende|Japanese\n",
      "who starred movies for the director of [Written on the Wind]\tSandra Dee|Charles Coburn|Cornel Wilde|John Gavin|Warren William|Susan Kohner|Joan Bennett|Fred MacMurray|Barbara Stanwyck|Don Ameche|Jane Wyman|Rochelle Hudson|Boris Karloff|Patricia Knight|Robert Cummings|Rock Hudson|Lucille Ball|Claudette Colbert|Lana Turner|George Sanders\n",
      "the films that share actors with the film [Creepshow] were in which languages\tPolish|English\n",
      "what were the release years of the films that share writers with the film [Grown Ups 2]\t1995|1996|1999|1998|1989|2002|2000|2008|2011|2010\n",
      "who is listed as director of the films starred by [The Inner Circle] actors\tJohn Landis|Atom Egoyan|Gary Trousdale|Steven Spielberg|Louis Leterrier|Alan Alda|Nora Ephron|Wolfgang Petersen|Allen Coulter|Richard Benjamin|Orson Welles|Neil Jordan|William Dieterle|Stephen Frears|Peter Yates|Markus Schleinzer|Wayne Wang|Jean Delannoy|Robert Zemeckis|Guy Jenkin|Christopher Hampton|Jack Clayton|Rocky Morton|Annabel Jankel|Mike Hodges|Kirk Wise|Ellen Perry|John Byrum|Abel Ferrara|Robert M. Young\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qa.answer_question(\n",
    "    \"the films that share actors with the film [Creepshow] were in which languages\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Zi72t_FVJeq",
    "outputId": "0a64956b-7d1b-4305-b816-4f56871b47db"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "the films that share actors with the film [Creepshow] were in which languages\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['English', 'Polish']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "qa.answer_question(\n",
    "    \"what are the languages of the films that have the same actors as [Creepshow]?\"\n",
    ")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rq3IBuOyZfeO",
    "outputId": "57ed5aa1-53df-49f4-d280-a9bb3e59ea91"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "what are the languages of the films that have the same actors as [Creepshow]?\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['English', 'Polish']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  }
 ]
}